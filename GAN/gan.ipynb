{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0819 15:43:26.239187 4430869952 deprecation.py:323] From <ipython-input-2-c3d55fec490c>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "W0819 15:43:26.240268 4430869952 deprecation.py:323] From /Users/skylar/anaconda3/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "W0819 15:43:26.242512 4430869952 deprecation.py:323] From /Users/skylar/anaconda3/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "W0819 15:44:01.987461 4430869952 deprecation.py:323] From /Users/skylar/anaconda3/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0819 15:44:03.024693 4430869952 deprecation.py:323] From /Users/skylar/anaconda3/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "W0819 15:44:03.027266 4430869952 deprecation.py:323] From /Users/skylar/anaconda3/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0819 15:44:08.123425 4430869952 deprecation.py:323] From /Users/skylar/anaconda3/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Params\n",
    "num_steps = 100000\n",
    "batch_size = 128\n",
    "learning_rate = 0.0002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network Params\n",
    "image_dim = 784 # 28*28 pixels\n",
    "gen_hidden_dim = 256\n",
    "disc_hidden_dim = 256\n",
    "noise_dim = 100 # Noise data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A custom initialization (see Xavier Glorot init)\n",
    "def glorot_init(shape):\n",
    "    return tf.random_normal(shape=shape, stddev=1. / tf.sqrt(shape[0] / 2.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'gen_hidden1': tf.Variable(glorot_init([noise_dim, gen_hidden_dim])),\n",
    "    'gen_out': tf.Variable(glorot_init([gen_hidden_dim, image_dim])),\n",
    "    'disc_hidden1': tf.Variable(glorot_init([image_dim, disc_hidden_dim])),\n",
    "    'disc_out': tf.Variable(glorot_init([disc_hidden_dim, 1])),\n",
    "}\n",
    "biases = {\n",
    "    'gen_hidden1': tf.Variable(tf.zeros([gen_hidden_dim])),\n",
    "    'gen_out': tf.Variable(tf.zeros([image_dim])),\n",
    "    'disc_hidden1': tf.Variable(tf.zeros([disc_hidden_dim])),\n",
    "    'disc_out': tf.Variable(tf.zeros([1])),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator\n",
    "def generator(x):\n",
    "    hidden_layer = tf.matmul(x, weights['gen_hidden1'])\n",
    "    hidden_layer = tf.add(hidden_layer, biases['gen_hidden1'])\n",
    "    hidden_layer = tf.nn.relu(hidden_layer)\n",
    "    out_layer = tf.matmul(hidden_layer, weights['gen_out'])\n",
    "    out_layer = tf.add(out_layer, biases['gen_out'])\n",
    "    out_layer = tf.nn.sigmoid(out_layer)\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator\n",
    "def discriminator(x):\n",
    "    hidden_layer = tf.matmul(x, weights['disc_hidden1'])\n",
    "    hidden_layer = tf.add(hidden_layer, biases['disc_hidden1'])\n",
    "    hidden_layer = tf.nn.relu(hidden_layer)\n",
    "    out_layer = tf.matmul(hidden_layer, weights['disc_out'])\n",
    "    out_layer = tf.add(out_layer, biases['disc_out'])\n",
    "    out_layer = tf.nn.sigmoid(out_layer)\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Networks\n",
    "# Network Inputs\n",
    "gen_input = tf.placeholder(tf.float32, shape=[None, noise_dim], name='input_noise')\n",
    "disc_input = tf.placeholder(tf.float32, shape=[None, image_dim], name='disc_input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Generator Network\n",
    "gen_sample = generator(gen_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build 2 Discriminator Networks (one from noise input, one from generated samples)\n",
    "disc_real = discriminator(disc_input)\n",
    "disc_fake = discriminator(gen_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Loss\n",
    "gen_loss = -tf.reduce_mean(tf.log(disc_fake))\n",
    "disc_loss = -tf.reduce_mean(tf.log(disc_real) + tf.log(1. - disc_fake))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Optimizers\n",
    "optimizer_gen = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "optimizer_disc = tf.train.AdamOptimizer(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Variables for each optimizer\n",
    "# By default in TensorFlow, all variables are updated by each optimizer, so we\n",
    "# need to precise for each one of them the specific variables to update.\n",
    "# Generator Network Variables\n",
    "gen_vars = [weights['gen_hidden1'], weights['gen_out'],\n",
    "            biases['gen_hidden1'], biases['gen_out']]\n",
    "# Discriminator Network Variables\n",
    "disc_vars = [weights['disc_hidden1'], weights['disc_out'],\n",
    "            biases['disc_hidden1'], biases['disc_out']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training operations\n",
    "train_gen = optimizer_gen.minimize(gen_loss, var_list=gen_vars)\n",
    "train_disc = optimizer_disc.minimize(disc_loss, var_list=disc_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Generator Loss: 1.161429, Discriminator Loss: 1.192188\n",
      "Step 1000: Generator Loss: 3.978533, Discriminator Loss: 0.051142\n",
      "Step 2000: Generator Loss: 4.680090, Discriminator Loss: 0.036851\n",
      "Step 3000: Generator Loss: 5.736567, Discriminator Loss: 0.014032\n",
      "Step 4000: Generator Loss: 3.268406, Discriminator Loss: 0.145238\n",
      "Step 5000: Generator Loss: 3.818455, Discriminator Loss: 0.116082\n",
      "Step 6000: Generator Loss: 4.517947, Discriminator Loss: 0.081008\n",
      "Step 7000: Generator Loss: 3.729537, Discriminator Loss: 0.152017\n",
      "Step 8000: Generator Loss: 4.199799, Discriminator Loss: 0.122079\n",
      "Step 9000: Generator Loss: 4.028124, Discriminator Loss: 0.150507\n",
      "Step 10000: Generator Loss: 4.205823, Discriminator Loss: 0.123198\n",
      "Step 11000: Generator Loss: 4.110412, Discriminator Loss: 0.132425\n",
      "Step 12000: Generator Loss: 3.808237, Discriminator Loss: 0.240700\n",
      "Step 13000: Generator Loss: 4.119029, Discriminator Loss: 0.324033\n",
      "Step 14000: Generator Loss: 4.168780, Discriminator Loss: 0.159984\n",
      "Step 15000: Generator Loss: 3.547846, Discriminator Loss: 0.317750\n",
      "Step 16000: Generator Loss: 3.624603, Discriminator Loss: 0.209229\n",
      "Step 17000: Generator Loss: 4.445380, Discriminator Loss: 0.141305\n",
      "Step 18000: Generator Loss: 3.157918, Discriminator Loss: 0.352832\n",
      "Step 19000: Generator Loss: 3.793496, Discriminator Loss: 0.250161\n",
      "Step 20000: Generator Loss: 3.627203, Discriminator Loss: 0.189478\n",
      "Step 21000: Generator Loss: 3.684106, Discriminator Loss: 0.362999\n",
      "Step 22000: Generator Loss: 3.544181, Discriminator Loss: 0.242799\n",
      "Step 23000: Generator Loss: 3.996511, Discriminator Loss: 0.267950\n",
      "Step 24000: Generator Loss: 2.842793, Discriminator Loss: 0.354742\n",
      "Step 25000: Generator Loss: 3.717962, Discriminator Loss: 0.346302\n",
      "Step 26000: Generator Loss: 3.029853, Discriminator Loss: 0.265167\n",
      "Step 27000: Generator Loss: 3.184509, Discriminator Loss: 0.297037\n",
      "Step 28000: Generator Loss: 3.353005, Discriminator Loss: 0.382762\n",
      "Step 29000: Generator Loss: 3.818494, Discriminator Loss: 0.465370\n",
      "Step 30000: Generator Loss: 3.027579, Discriminator Loss: 0.380689\n",
      "Step 31000: Generator Loss: 3.082414, Discriminator Loss: 0.333749\n",
      "Step 32000: Generator Loss: 2.915809, Discriminator Loss: 0.483557\n",
      "Step 33000: Generator Loss: 3.424108, Discriminator Loss: 0.317153\n",
      "Step 34000: Generator Loss: 2.994843, Discriminator Loss: 0.358450\n",
      "Step 35000: Generator Loss: 2.815476, Discriminator Loss: 0.458575\n",
      "Step 36000: Generator Loss: 2.956887, Discriminator Loss: 0.439235\n",
      "Step 37000: Generator Loss: 2.616238, Discriminator Loss: 0.506761\n",
      "Step 38000: Generator Loss: 2.887530, Discriminator Loss: 0.545076\n",
      "Step 39000: Generator Loss: 3.090294, Discriminator Loss: 0.356768\n",
      "Step 40000: Generator Loss: 2.858350, Discriminator Loss: 0.433393\n",
      "Step 41000: Generator Loss: 2.966309, Discriminator Loss: 0.455582\n",
      "Step 42000: Generator Loss: 2.940085, Discriminator Loss: 0.465549\n",
      "Step 43000: Generator Loss: 2.934813, Discriminator Loss: 0.524955\n",
      "Step 44000: Generator Loss: 2.855989, Discriminator Loss: 0.488112\n",
      "Step 45000: Generator Loss: 2.652224, Discriminator Loss: 0.432653\n",
      "Step 46000: Generator Loss: 2.668959, Discriminator Loss: 0.523300\n",
      "Step 47000: Generator Loss: 2.658919, Discriminator Loss: 0.379870\n",
      "Step 48000: Generator Loss: 2.995553, Discriminator Loss: 0.414801\n",
      "Step 49000: Generator Loss: 2.610791, Discriminator Loss: 0.616348\n",
      "Step 50000: Generator Loss: 2.643201, Discriminator Loss: 0.464562\n",
      "Step 51000: Generator Loss: 2.634872, Discriminator Loss: 0.452831\n",
      "Step 52000: Generator Loss: 2.796924, Discriminator Loss: 0.382826\n",
      "Step 53000: Generator Loss: 2.940277, Discriminator Loss: 0.376380\n",
      "Step 54000: Generator Loss: 2.691988, Discriminator Loss: 0.510630\n",
      "Step 55000: Generator Loss: 2.599307, Discriminator Loss: 0.555584\n",
      "Step 56000: Generator Loss: 2.734646, Discriminator Loss: 0.524069\n",
      "Step 57000: Generator Loss: 2.549868, Discriminator Loss: 0.465683\n",
      "Step 58000: Generator Loss: 2.946959, Discriminator Loss: 0.441288\n",
      "Step 59000: Generator Loss: 3.022189, Discriminator Loss: 0.423122\n",
      "Step 60000: Generator Loss: 2.810149, Discriminator Loss: 0.407208\n",
      "Step 61000: Generator Loss: 3.130634, Discriminator Loss: 0.537262\n",
      "Step 62000: Generator Loss: 2.990700, Discriminator Loss: 0.497340\n",
      "Step 63000: Generator Loss: 3.163660, Discriminator Loss: 0.394493\n",
      "Step 64000: Generator Loss: 3.225383, Discriminator Loss: 0.433609\n",
      "Step 65000: Generator Loss: 2.642509, Discriminator Loss: 0.356280\n",
      "Step 66000: Generator Loss: 2.740166, Discriminator Loss: 0.474488\n",
      "Step 67000: Generator Loss: 3.011375, Discriminator Loss: 0.399823\n",
      "Step 68000: Generator Loss: 2.433221, Discriminator Loss: 0.476281\n",
      "Step 69000: Generator Loss: 3.201913, Discriminator Loss: 0.432022\n",
      "Step 70000: Generator Loss: 3.229993, Discriminator Loss: 0.482494\n",
      "Step 71000: Generator Loss: 2.652682, Discriminator Loss: 0.423383\n",
      "Step 72000: Generator Loss: 2.970765, Discriminator Loss: 0.442535\n",
      "Step 73000: Generator Loss: 2.719009, Discriminator Loss: 0.451952\n",
      "Step 74000: Generator Loss: 2.737842, Discriminator Loss: 0.364301\n",
      "Step 75000: Generator Loss: 2.797693, Discriminator Loss: 0.495042\n",
      "Step 76000: Generator Loss: 3.196435, Discriminator Loss: 0.345648\n",
      "Step 77000: Generator Loss: 2.990141, Discriminator Loss: 0.351561\n",
      "Step 78000: Generator Loss: 2.795944, Discriminator Loss: 0.412753\n",
      "Step 79000: Generator Loss: 2.531547, Discriminator Loss: 0.369579\n",
      "Step 80000: Generator Loss: 2.863426, Discriminator Loss: 0.385031\n",
      "Step 81000: Generator Loss: nan, Discriminator Loss: nan\n",
      "Step 82000: Generator Loss: nan, Discriminator Loss: nan\n",
      "Step 83000: Generator Loss: nan, Discriminator Loss: nan\n",
      "Step 84000: Generator Loss: nan, Discriminator Loss: nan\n",
      "Step 85000: Generator Loss: nan, Discriminator Loss: nan\n",
      "Step 86000: Generator Loss: nan, Discriminator Loss: nan\n",
      "Step 87000: Generator Loss: nan, Discriminator Loss: nan\n",
      "Step 88000: Generator Loss: nan, Discriminator Loss: nan\n",
      "Step 89000: Generator Loss: nan, Discriminator Loss: nan\n",
      "Step 90000: Generator Loss: nan, Discriminator Loss: nan\n",
      "Step 91000: Generator Loss: nan, Discriminator Loss: nan\n",
      "Step 92000: Generator Loss: nan, Discriminator Loss: nan\n",
      "Step 93000: Generator Loss: nan, Discriminator Loss: nan\n",
      "Step 94000: Generator Loss: nan, Discriminator Loss: nan\n",
      "Step 95000: Generator Loss: nan, Discriminator Loss: nan\n",
      "Step 96000: Generator Loss: nan, Discriminator Loss: nan\n",
      "Step 97000: Generator Loss: nan, Discriminator Loss: nan\n",
      "Step 98000: Generator Loss: nan, Discriminator Loss: nan\n",
      "Step 99000: Generator Loss: nan, Discriminator Loss: nan\n",
      "Step 100000: Generator Loss: nan, Discriminator Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/skylar/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:39: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      "/Users/skylar/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:41: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "\n",
    "    for i in range(1, num_steps+1):\n",
    "        # Prepare Data\n",
    "        # Get the next batch of MNIST data (only images are needed, not labels)\n",
    "        batch_x, _ = mnist.train.next_batch(batch_size)\n",
    "        # Generate noise to feed to the generator\n",
    "        z = np.random.uniform(-1., 1., size=[batch_size, noise_dim])\n",
    "\n",
    "        # Train\n",
    "        feed_dict = {disc_input: batch_x, gen_input: z}\n",
    "        _, _, gl, dl = sess.run([train_gen, train_disc, gen_loss, disc_loss],\n",
    "                                feed_dict=feed_dict)\n",
    "        if i % 1000 == 0 or i == 1:\n",
    "            print('Step %i: Generator Loss: %f, Discriminator Loss: %f' % (i, gl, dl))\n",
    "\n",
    "    # Generate images from noise, using the generator network.\n",
    "    f, a = plt.subplots(4, 10, figsize=(10, 4))\n",
    "    for i in range(10):\n",
    "        # Noise input.\n",
    "        z = np.random.uniform(-1., 1., size=[4, noise_dim])\n",
    "        g = sess.run([gen_sample], feed_dict={gen_input: z})\n",
    "        g = np.reshape(g, newshape=(4, 28, 28, 1))\n",
    "        # Reverse colours for better display\n",
    "        g = -1 * (g - 1)\n",
    "        for j in range(4):\n",
    "            # Generate image from noise. Extend to 3 channels for matplot figure.\n",
    "            img = np.reshape(np.repeat(g[j][:, :, np.newaxis], 3, axis=2),\n",
    "                             newshape=(28, 28, 3))\n",
    "            a[j][i].imshow(img)\n",
    "\n",
    "    f.show()\n",
    "    plt.draw()\n",
    "    plt.waitforbuttonpress()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
